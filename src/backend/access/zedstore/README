
src/backend/access/zedstore/README

ZedStore - compressed column (and row) store for PostgreSQL
===========================================================

The purpose of this README is to provide overview of zedstore's
design, major requirements/objectives it intends to fulfill and
high-level implementation details.

Objectives
----------

* Performance improvement for queries selecting subset of columns
(reduced IO).

* Reduced on-disk footprint compared to heap table. Shorter tuple
headers and also leveraging compression of similar type data

* Be first-class citizen in the Postgres architecture (tables data can
just independently live in columnar storage) and not be at arm's
length though an opaque interface.

* Fully MVCC compliant - basically all operations supported similar to
heap, like update, delete, serializable transactions etc...

* All Indexes supported

* Hybrid row-column store, where some columns are stored together, and
others separately. Provide flexibility of granularity on how to divide
the columns. Columns accessed together can be stored together.

* Provide better control over bloat (using zheap)

* Eliminate need for separate toast tables

* Faster add / drop column or changing data type of column by avoiding
full rewrite of the table.

Highlevel design of zedStore - B-trees for the win!
---------------------------------------------------

To start simple, let's ignore column store aspect and consider it as
compressed row store. The column store is natural externsion of this
concept, explained in next section.

The basic on-disk data structure leveraged is a B-tree, indexed by
TID. BTree being a great data structure, fast and versatile. Note this
is not refering to existing Btree indexes, but instead net new BTree
for table data storage.

TID - used as a logical row identifier:
TID is just a 48-bit row identifier. The traditional division into
block and offset numbers is meaningless. In order to find a tuple with
a given TID, one must always descend the B-tree. Having logical TID
provides flexibility to move the tuples around different pages on page
splits or page merges can be performed.

The internal pages of the B-tree are super simple and boring. Each
internal page just stores an array of TID/downlink pairs. Let's focus
on the leaf level. Leaf blocks have short uncompressed header,
followed by btree items. It contains three kind of items:

- plain item, holds one tuple or one datum, uncompressed payload

- array item, holds multiple datums, with consecutive TIDs and the
same visibility information. An array item saves space compared to
multiple single items, by leaving out repetitive UNDO and TID
fields. An array item cannot mix NULLs and non-NULLs, so the ZSBT_NULL
flag applies to all elements.

- a "container item", holds multiple plain items, compressed payload

+-----------------------------
| Fixed-size page header:
|
|   LSN
|   TID low and hi key (for Lehman & Yao B-tree operations)
|   left and right page pointers
|
| Items:
|
|   TID | size | flags | uncompressed size | lastTID | payload (container item)
|   TID | size | flags | uncompressed size | lastTID | payload (container item)
|   TID | size | flags | undo pointer | payload (plain item)
|   TID | size | flags | undo pointer | payload (plain item)
|   ...
|
+----------------------------

Row store
---------

The tuples are stored one after another, sorted by TID. For each
tuple, we store its 48-bit TID, a undo record pointer, and the actual
tuple data uncompressed.

In uncompressed form, the page can be arbitrarily large. But after
compression, it must fit into a physical 8k block. If on insert or
update of a tuple, the page cannot be compressed below 8k anymore, the
page is split. Note that because TIDs are logical rather than physical
identifiers, we can freely move tuples from one physical page to
another during page split. A tuple's TID never changes.

The buffer cache caches compressed blocks. Likewise, WAL-logging,
full-page images etc. work on compressed blocks. Uncompression is done
on-the-fly, as and when needed in backend-private memory, when
reading. For some compressions like rel encoding or delta encoding
tuples can be constructed directly from compressed data.

Column store
------------

A column store uses the same structure but we have *multiple* B-trees,
one for each column plus one for storing meta-data
(a.k.a. meta-column), all indexed by TID. Imagine zedstore as a forest
of B-trees. The B-trees for all columns are stored in the same
physical file.

A metapage at block 0, has links to the roots of the B-trees. Leaf
pages look the same, but instead of storing the whole tuple, stores
just a single attribute. To reconstruct a row with given TID, scan
descends down the B-trees for all the columns using that TID, and
fetches all attributes. Likewise, a sequential scan walks all the
B-trees in lockstep.

The special or first btree for meta-column is used to allocate TIDs
for tuples, track the UNDO location which provides visibility
information. Also this special btree, which always exists, helps to
support zero column tables (which can be result of ADD COLUMN DROP
COLUMN actions as well). Plus, having meta-data stored separately from
data, helps to get better compression ratios. And also helps to
simplify the overall design/implementation as for deletes just need to
edit the meta-column and avoid touching the actual data btrees.


MVCC
----

Undo record pointers are used to implement MVCC, like in zheap. Hence,
transaction information if not directly stored with the data. In
zheap, there's a small, fixed, number of "transaction slots" on each
page, but zedstore has undo pointer with each item directly; in normal
cases, the compression squeezes this down to almost nothing. In case
of bulk load the undo record pointer is maintained for array of items
and not per item. Undo pointer is only stored in meta-column and all
MVCC operations are performed using the meta-column only.


Insert:
Inserting a new row, splits the row into datums. Then while adding
entry for meta-column adds, decides block to insert, picks a TID for
it, and writes undo record for the same. All the data columns are
inserted using that TID.

Toast:
When an overly large datum is stored, it is divided into chunks, and
each chunk is stored on a dedicated toast page within the same
physical file. The toast pages of a datum form list, each page has a
next/prev pointer.

Select:
Property is added to Table AM to convey if column projection is
leveraged by AM for scans. While scanning tables with AM leveraging
this property, executor parses the plan. Leverages the target list and
quals to find the required columns for query. This list is passed down
to AM on beginscan. Zedstore uses this column projection list to only
pull data from selected columns. Virtual tuple table slot is used to
pass back the datums for subset of columns.

Current table am API requires enhancement here to pass down column
projection to AM. The patch showcases two different ways for the same.

* For sequential scans added new beginscan_with_column_projection()
API. Executor checks AM property and if it leverages column projection
uses this new API else normal beginscan() API.

* For index scans instead of modifying the begin scan API, added new
API to specifically pass column projection list after calling begin
scan to populate the scan descriptor but before fetching the tuples.

Delete:
When deleting a tuple, new undo record is created for delete and only
meta-column item is updated with this new undo record. New undo record
created points to previous undo record pointer (insert undo record)
present for the tuple. Hence, delete only operates on meta-column and
no data column is edited.

Update:
Update in zedstore is pretty equivalent to delete and insert. Delete
action is performed as stated above and new entry is added with
updated values. So, no in-place update happens.

Index Support:
Building index also leverages columnar storage and only scans columns
required to build the index. Indexes work pretty similar to heap
tables. Data is inserted into tables and TID for the tuple gets stored
in index. On index scans, required column Btrees are scanned for given
TID and datums passed back using virtual tuple. Since only meta-column
is leveraged to perform visibility check, only visible tuples data are
fetched from rest of the Btrees.

Page Format
-----------
A ZedStore table contains different kinds of pages, all in the same
file. Kinds of pages are meta-page, per-attribute btree internal and
leaf pages, UNDO log page, and toast pages. Each page type has its own
distinct data storage format.

META Page:
Block 0 is always a metapage. It contains the block numbers of the
other data structures stored within the file, like the per-attribute
B-trees, and the UNDO log.

BTREE Page:

UNDO Page:

TOAST Page:


Free Space Map
--------------


Enhancements
------------

Instead of compressing all the tuples on a page in one batch, store a
small "dictionary", e.g. in page header or meta page or separate
dedicated page, and use it to compress tuple by tuple. That could make
random reads and updates of individual tuples faster. Need to find how
to create the dictionary first.

Only cached compressed pages in the page cache. If we want to cache
uncompressed pages instead, or in addition to that, we need to invent
a whole new kind of a buffer cache that can deal with the
variable-size blocks. For a first version, I think we can live without
it.

Instead of storing all columns in the same file, we could store them
in separate files (separate forks?). That would allow immediate reuse
of space, after dropping a column. It's not clear how to use an FSM in
that case, though. Might have to implement an integrated FSM,
too. (Which might not be a bad idea, anyway).

Design allows for hybrid row-column store, where some columns are
stored together, and others have a dedicated B-tree. Need to have user
facing syntax to allow specifying how to group the columns.

Salient points for the design
------------------------------

* Layout the data/tuples in mapped fashion instead of keeping the
logical to physical mapping separate from actual data. So, keep all
the meta-data and data logically in single stream of file, avoiding
the need for separate forks/files to store meta-data and data.

* Handle/treat operations at tuple level and not block level.

* Stick to fixed size physical blocks. Variable size blocks (for
possibly higher compression ratios) pose need for increased logical to
physical mapping maintenance, plus restrictions on concurrency of
writes and reads to files. Hence adopt compression to fit fixed size
blocks instead of other way round.


Predicate locking
-----------------

Predicate locks, to support SERIALIZABLE transactinons, are taken like
with the heap. From README-SSI:

* For a table scan, the entire relation will be locked.

* Each tuple read which is visible to the reading transaction will be
locked, whether or not it meets selection criteria; except that there
is no need to acquire an SIREAD lock on a tuple when the transaction
already holds a write lock on any tuple representing the row, since a
rw-conflict would also create a ww-dependency which has more
aggressive enforcement and thus will prevent any anomaly.

* Modifying a heap tuple creates a rw-conflict with any transaction
that holds a SIREAD lock on that tuple, or on the page or relation
that contains it.

* Inserting a new tuple creates a rw-conflict with any transaction
holding a SIREAD lock on the entire relation. It doesn't conflict with
page-level locks, because page-level locks are only used to aggregate
tuple locks. Unlike index page locks, they don't lock "gaps" on the
page.


ZedStore isn't block-based, so page-level locks really just mean a
range of TIDs. They're only used to aggregate tuple locks.
